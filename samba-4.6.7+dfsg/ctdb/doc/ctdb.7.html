<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>ctdb</title><meta name="generator" content="DocBook XSL Stylesheets V1.79.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="refentry"><a name="ctdb.7"></a><div class="titlepage"></div><div class="refnamediv"><h2>Name</h2><p>ctdb &#8212; Clustered TDB</p></div><div class="refsect1"><a name="idm10"></a><h2>DESCRIPTION</h2><p>
    CTDB is a clustered database component in clustered Samba that
    provides a high-availability load-sharing CIFS server cluster.
  </p><p>
    The main functions of CTDB are:
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
	Provide a clustered version of the TDB database with automatic
	rebuild/recovery of the databases upon node failures.
      </p></li><li class="listitem"><p>
      Monitor nodes in the cluster and services running on each node.
      </p></li><li class="listitem"><p>
	Manage a pool of public IP addresses that are used to provide
	services to clients.  Alternatively, CTDB can be used with
	LVS.
      </p></li></ul></div><p>
    Combined with a cluster filesystem CTDB provides a full
    high-availablity (HA) environment for services such as clustered
    Samba, NFS and other services.
  </p></div><div class="refsect1"><a name="idm22"></a><h2>ANATOMY OF A CTDB CLUSTER</h2><p>
    A CTDB cluster is a collection of nodes with 2 or more network
    interfaces.  All nodes provide network (usually file/NAS) services
    to clients.  Data served by file services is stored on shared
    storage (usually a cluster filesystem) that is accessible by all
    nodes.
  </p><p>
    CTDB provides an "all active" cluster, where services are load
    balanced across all nodes.
  </p></div><div class="refsect1"><a name="idm26"></a><h2>Recovery Lock</h2><p>
      CTDB uses a <span class="emphasis"><em>recovery lock</em></span> to avoid a
      <span class="emphasis"><em>split brain</em></span>, where a cluster becomes
      partitioned and each partition attempts to operate
      independently.  Issues that can result from a split brain
      include file data corruption, because file locking metadata may
      not be tracked correctly.
    </p><p>
      CTDB uses a <span class="emphasis"><em>cluster leader and follower</em></span>
      model of cluster management.  All nodes in a cluster elect one
      node to be the leader.  The leader node coordinates privileged
      operations such as database recovery and IP address failover.
      CTDB refers to the leader node as the <span class="emphasis"><em>recovery
      master</em></span>.  This node takes and holds the recovery lock
      to assert its privileged role in the cluster.
    </p><p>
      By default, the recovery lock is implemented using a file
      (specified by <em class="parameter"><code>CTDB_RECOVERY_LOCK</code></em>)
      residing in shared storage (usually) on a cluster filesystem.
      To support a recovery lock the cluster filesystem must support
      lock coherence.  See
      <span class="citerefentry"><span class="refentrytitle">ping_pong</span>(1)</span> for more details.
    </p><p>
      The recovery lock can also be implemented using an arbitrary
      cluster mutex call-out by using an exclamation point ('!') as
      the first character of
      <em class="parameter"><code>CTDB_RECOVERY_LOCK</code></em>.  For example, a value
      of <span class="command"><strong>!/usr/local/bin/myhelper recovery</strong></span> would
      run the given helper with the specified arguments.  See the
      source code relating to cluster mutexes for clues about writing
      call-outs.
    </p><p>
      If a cluster becomes partitioned (for example, due to a
      communication failure) and a different recovery master is
      elected by the nodes in each partition, then only one of these
      recovery masters will be able to take the recovery lock.  The
      recovery master in the "losing" partition will not be able to
      take the recovery lock and will be excluded from the cluster.
      The nodes in the "losing" partition will elect each node in turn
      as their recovery master so eventually all the nodes in that
      partition will be excluded.
    </p><p>
      CTDB does sanity checks to ensure that the recovery lock is held
      as expected.
    </p><p>
      CTDB can run without a recovery lock but this is not recommended
      as there will be no protection from split brains.
    </p></div><div class="refsect1"><a name="idm45"></a><h2>Private vs Public addresses</h2><p>
      Each node in a CTDB cluster has multiple IP addresses assigned
      to it:

      </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
	    A single private IP address that is used for communication
	    between nodes.
	  </p></li><li class="listitem"><p>
	    One or more public IP addresses that are used to provide
	    NAS or other services.
	  </p></li></ul></div><p>
    </p><div class="refsect2"><a name="idm53"></a><h3>Private address</h3><p>
        Each node is configured with a unique, permanently assigned
        private address.  This address is configured by the operating
        system.  This address uniquely identifies a physical node in
        the cluster and is the address that CTDB daemons will use to
        communicate with the CTDB daemons on other nodes.
      </p><p>
        Private addresses are listed in the file specified by the
        <code class="varname">CTDB_NODES</code> configuration variable (see
        <span class="citerefentry"><span class="refentrytitle">ctdbd.conf</span>(5)</span>, default
        <code class="filename">/usr/local/etc/ctdb/nodes</code>).  This file contains the
        list of private addresses for all nodes in the cluster, one
        per line. This file must be the same on all nodes in the
        cluster.
      </p><p>
	Private addresses should not be used by clients to connect to
	services provided by the cluster.
      </p><p>
        It is strongly recommended that the private addresses are
        configured on a private network that is separate from client
        networks.  This is because the CTDB protocol is both
        unauthenticated and unencrypted.  If clients share the private
        network then steps need to be taken to stop injection of
        packets to relevant ports on the private addresses.  It is
        also likely that CTDB protocol traffic between nodes could
        leak sensitive information if it can be intercepted.
      </p><p>
	Example <code class="filename">/usr/local/etc/ctdb/nodes</code> for a four node
	cluster:
      </p><pre class="screen">
192.168.1.1
192.168.1.2
192.168.1.3
192.168.1.4
      </pre></div><div class="refsect2"><a name="idm67"></a><h3>Public addresses</h3><p>
	Public addresses are used to provide services to clients.
	Public addresses are not configured at the operating system
	level and are not permanently associated with a particular
	node.  Instead, they are managed by CTDB and are assigned to
	interfaces on physical nodes at runtime.
      </p><p>
        The CTDB cluster will assign/reassign these public addresses
        across the available healthy nodes in the cluster. When one
        node fails, its public addresses will be taken over by one or
        more other nodes in the cluster.  This ensures that services
        provided by all public addresses are always available to
        clients, as long as there are nodes available capable of
        hosting this address.
      </p><p>
	The public address configuration is stored in a file on each
	node specified by the <code class="varname">CTDB_PUBLIC_ADDRESSES</code>
	configuration variable (see
	<span class="citerefentry"><span class="refentrytitle">ctdbd.conf</span>(5)</span>, recommended
	<code class="filename">/usr/local/etc/ctdb/public_addresses</code>).  This file
	contains a list of the public addresses that the node is
	capable of hosting, one per line.  Each entry also contains
	the netmask and the interface to which the address should be
	assigned.
      </p><p>
	Example <code class="filename">/usr/local/etc/ctdb/public_addresses</code> for a
	node that can host 4 public addresses, on 2 different
	interfaces:
      </p><pre class="screen">
10.1.1.1/24 eth1
10.1.1.2/24 eth1
10.1.2.1/24 eth2
10.1.2.2/24 eth2
      </pre><p>
	In many cases the public addresses file will be the same on
	all nodes.  However, it is possible to use different public
	address configurations on different nodes.
      </p><p>
	Example: 4 nodes partitioned into two subgroups:
      </p><pre class="screen">
Node 0:/usr/local/etc/ctdb/public_addresses
	10.1.1.1/24 eth1
	10.1.1.2/24 eth1

Node 1:/usr/local/etc/ctdb/public_addresses
	10.1.1.1/24 eth1
	10.1.1.2/24 eth1

Node 2:/usr/local/etc/ctdb/public_addresses
	10.1.2.1/24 eth2
	10.1.2.2/24 eth2

Node 3:/usr/local/etc/ctdb/public_addresses
	10.1.2.1/24 eth2
	10.1.2.2/24 eth2
      </pre><p>
	In this example nodes 0 and 1 host two public addresses on the
	10.1.1.x network while nodes 2 and 3 host two public addresses
	for the 10.1.2.x network.
      </p><p>
	Public address 10.1.1.1 can be hosted by either of nodes 0 or
	1 and will be available to clients as long as at least one of
	these two nodes are available.
      </p><p>
	If both nodes 0 and 1 become unavailable then public address
	10.1.1.1 also becomes unavailable. 10.1.1.1 can not be failed
	over to nodes 2 or 3 since these nodes do not have this public
	address configured.
      </p><p>
        The <span class="command"><strong>ctdb ip</strong></span> command can be used to view the
        current assignment of public addresses to physical nodes.
      </p></div></div><div class="refsect1"><a name="idm88"></a><h2>Node status</h2><p>
      The current status of each node in the cluster can be viewed by the 
      <span class="command"><strong>ctdb status</strong></span> command.
    </p><p>
      A node can be in one of the following states:
    </p><div class="variablelist"><dl class="variablelist"><dt><span class="term">OK</span></dt><dd><p>
	    This node is healthy and fully functional.  It hosts public
	    addresses to provide services.
	  </p></dd><dt><span class="term">DISCONNECTED</span></dt><dd><p>
	    This node is not reachable by other nodes via the private
	    network.  It is not currently participating in the cluster.
	    It <span class="emphasis"><em>does not</em></span> host public addresses to
	    provide services.  It might be shut down.
	  </p></dd><dt><span class="term">DISABLED</span></dt><dd><p>
	    This node has been administratively disabled. This node is
	    partially functional and participates in the cluster.
	    However, it <span class="emphasis"><em>does not</em></span> host public
	    addresses to provide services.
	  </p></dd><dt><span class="term">UNHEALTHY</span></dt><dd><p>
	    A service provided by this node has failed a health check
	    and should be investigated.  This node is partially
	    functional and participates in the cluster.  However, it
	    <span class="emphasis"><em>does not</em></span> host public addresses to
	    provide services.  Unhealthy nodes should be investigated
	    and may require an administrative action to rectify.
	  </p></dd><dt><span class="term">BANNED</span></dt><dd><p>
	    CTDB is not behaving as designed on this node.  For example,
	    it may have failed too many recovery attempts.  Such nodes
	    are banned from participating in the cluster for a
	    configurable time period before they attempt to rejoin the
	    cluster.  A banned node <span class="emphasis"><em>does not</em></span> host
	    public addresses to provide services.  All banned nodes
	    should be investigated and may require an administrative
	    action to rectify.
	  </p></dd><dt><span class="term">STOPPED</span></dt><dd><p>
	    This node has been administratively exclude from the
	    cluster.  A stopped node does no participate in the cluster
	    and <span class="emphasis"><em>does not</em></span> host public addresses to
	    provide services.  This state can be used while performing
	    maintenance on a node.
	  </p></dd><dt><span class="term">PARTIALLYONLINE</span></dt><dd><p>
	    A node that is partially online participates in a cluster
	    like a healthy (OK) node.  Some interfaces to serve public
	    addresses are down, but at least one interface is up.  See
	    also <span class="command"><strong>ctdb ifaces</strong></span>.
	  </p></dd></dl></div></div><div class="refsect1"><a name="idm128"></a><h2>CAPABILITIES</h2><p>
      Cluster nodes can have several different capabilities enabled.
      These are listed below.
    </p><div class="variablelist"><dl class="variablelist"><dt><span class="term">RECMASTER</span></dt><dd><p>
	    Indicates that a node can become the CTDB cluster recovery
	    master.  The current recovery master is decided via an
	    election held by all active nodes with this capability.
	  </p><p>
	    Default is YES.
	  </p></dd><dt><span class="term">LMASTER</span></dt><dd><p>
	    Indicates that a node can be the location master (LMASTER)
	    for database records.  The LMASTER always knows which node
	    has the latest copy of a record in a volatile database.
	  </p><p>
	    Default is YES.
	  </p></dd></dl></div><p>
      The RECMASTER and LMASTER capabilities can be disabled when CTDB
      is used to create a cluster spanning across WAN links. In this
      case CTDB acts as a WAN accelerator.
    </p></div><div class="refsect1"><a name="idm143"></a><h2>LVS</h2><p>
      LVS is a mode where CTDB presents one single IP address for the
      entire cluster. This is an alternative to using public IP
      addresses and round-robin DNS to loadbalance clients across the
      cluster.
    </p><p>
      This is similar to using a layer-4 loadbalancing switch but with
      some restrictions.
    </p><p>
      One extra LVS public address is assigned on the public network
      to each LVS group.  Each LVS group is a set of nodes in the
      cluster that presents the same LVS address public address to the
      outside world.  Normally there would only be one LVS group
      spanning an entire cluster, but in situations where one CTDB
      cluster spans multiple physical sites it might be useful to have
      one LVS group for each site.  There can be multiple LVS groups
      in a cluster but each node can only be member of one LVS group.
    </p><p>
      Client access to the cluster is load-balanced across the HEALTHY
      nodes in an LVS group.  If no HEALTHY nodes exists then all
      nodes in the group are used, regardless of health status.  CTDB
      will, however never load-balance LVS traffic to nodes that are
      BANNED, STOPPED, DISABLED or DISCONNECTED.  The <span class="command"><strong>ctdb
      lvs</strong></span> command is used to show which nodes are currently
      load-balanced across.
    </p><p>
      In each LVS group, one of the nodes is selected by CTDB to be
      the LVS master.  This node receives all traffic from clients
      coming in to the LVS public address and multiplexes it across
      the internal network to one of the nodes that LVS is using.
      When responding to the client, that node will send the data back
      directly to the client, bypassing the LVS master node.  The
      command <span class="command"><strong>ctdb lvsmaster</strong></span> will show which node
      is the current LVS master.
    </p><p>
      The path used for a client I/O is:
      </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
	    Client sends request packet to LVSMASTER.
	  </p></li><li class="listitem"><p>
	    LVSMASTER passes the request on to one node across the
	    internal network.
	  </p></li><li class="listitem"><p>
	    Selected node processes the request.
	  </p></li><li class="listitem"><p>
	    Node responds back to client.
	  </p></li></ol></div><p>
    </p><p>
      This means that all incoming traffic to the cluster will pass
      through one physical node, which limits scalability. You can
      send more data to the LVS address that one physical node can
      multiplex. This means that you should not use LVS if your I/O
      pattern is write-intensive since you will be limited in the
      available network bandwidth that node can handle.  LVS does work
      wery well for read-intensive workloads where only smallish READ
      requests are going through the LVSMASTER bottleneck and the
      majority of the traffic volume (the data in the read replies)
      goes straight from the processing node back to the clients. For
      read-intensive i/o patterns you can acheive very high throughput
      rates in this mode.
    </p><p>
      Note: you can use LVS and public addresses at the same time.
    </p><p>
      If you use LVS, you must have a permanent address configured for
      the public interface on each node. This address must be routable
      and the cluster nodes must be configured so that all traffic
      back to client hosts are routed through this interface. This is
      also required in order to allow samba/winbind on the node to
      talk to the domain controller.  This LVS IP address can not be
      used to initiate outgoing traffic.
    </p><p>
      Make sure that the domain controller and the clients are
      reachable from a node <span class="emphasis"><em>before</em></span> you enable
      LVS.  Also ensure that outgoing traffic to these hosts is routed
      out through the configured public interface.
    </p><div class="refsect2"><a name="idm167"></a><h3>Configuration</h3><p>
	To activate LVS on a CTDB node you must specify the
	<code class="varname">CTDB_LVS_PUBLIC_IFACE</code>,
	<code class="varname">CTDB_LVS_PUBLIC_IP</code> and
	<code class="varname">CTDB_LVS_NODES</code> configuration variables.
	<code class="varname">CTDB_LVS_NODES</code> specifies a file containing
	the private address of all nodes in the current node's LVS
	group.
      </p><p>
	Example:
	</p><pre class="screen">
CTDB_LVS_PUBLIC_IFACE=eth1
CTDB_LVS_PUBLIC_IP=10.1.1.237
CTDB_LVS_NODES=/usr/local/etc/ctdb/lvs_nodes
	</pre><p>
      </p><p>
	Example <code class="filename">/usr/local/etc/ctdb/lvs_nodes</code>:
      </p><pre class="screen">
192.168.1.2
192.168.1.3
192.168.1.4
      </pre><p>
	Normally any node in an LVS group can act as the LVS master.
	Nodes that are highly loaded due to other demands maybe
	flagged with the "slave-only" option in the
	<code class="varname">CTDB_LVS_NODES</code> file to limit the LVS
	functionality of those nodes.
      </p><p>
	LVS nodes file that excludes 192.168.1.4 from being
	the LVS master node:
      </p><pre class="screen">
192.168.1.2
192.168.1.3
192.168.1.4 slave-only
      </pre></div></div><div class="refsect1"><a name="idm183"></a><h2>TRACKING AND RESETTING TCP CONNECTIONS</h2><p>
      CTDB tracks TCP connections from clients to public IP addresses,
      on known ports.  When an IP address moves from one node to
      another, all existing TCP connections to that IP address are
      reset.  The node taking over this IP address will also send
      gratuitous ARPs (for IPv4, or neighbour advertisement, for
      IPv6).  This allows clients to reconnect quickly, rather than
      waiting for TCP timeouts, which can be very long.
    </p><p>
      It is important that established TCP connections do not survive
      a release and take of a public IP address on the same node.
      Such connections can get out of sync with sequence and ACK
      numbers, potentially causing a disruptive ACK storm.
    </p></div><div class="refsect1"><a name="idm187"></a><h2>NAT GATEWAY</h2><p>
      NAT gateway (NATGW) is an optional feature that is used to
      configure fallback routing for nodes.  This allows cluster nodes
      to connect to external services (e.g. DNS, AD, NIS and LDAP)
      when they do not host any public addresses (e.g. when they are
      unhealthy).
    </p><p>
      This also applies to node startup because CTDB marks nodes as
      UNHEALTHY until they have passed a "monitor" event.  In this
      context, NAT gateway helps to avoid a "chicken and egg"
      situation where a node needs to access an external service to
      become healthy.
    </p><p>
      Another way of solving this type of problem is to assign an
      extra static IP address to a public interface on every node.
      This is simpler but it uses an extra IP address per node, while
      NAT gateway generally uses only one extra IP address.
    </p><div class="refsect2"><a name="idm192"></a><h3>Operation</h3><p>
	One extra NATGW public address is assigned on the public
	network to each NATGW group.  Each NATGW group is a set of
	nodes in the cluster that shares the same NATGW address to
	talk to the outside world.  Normally there would only be one
	NATGW group spanning an entire cluster, but in situations
	where one CTDB cluster spans multiple physical sites it might
	be useful to have one NATGW group for each site.
      </p><p>
	There can be multiple NATGW groups in a cluster but each node
	can only be member of one NATGW group.
      </p><p>
	In each NATGW group, one of the nodes is selected by CTDB to
	be the NATGW master and the other nodes are consider to be
	NATGW slaves.  NATGW slaves establish a fallback default route
	to the NATGW master via the private network.  When a NATGW
	slave hosts no public IP addresses then it will use this route
	for outbound connections.  The NATGW master hosts the NATGW
	public IP address and routes outgoing connections from
	slave nodes via this IP address.  It also establishes a
	fallback default route.
      </p></div><div class="refsect2"><a name="idm197"></a><h3>Configuration</h3><p>
	NATGW is usually configured similar to the following example configuration:
      </p><pre class="screen">
CTDB_NATGW_NODES=/usr/local/etc/ctdb/natgw_nodes
CTDB_NATGW_PRIVATE_NETWORK=192.168.1.0/24
CTDB_NATGW_PUBLIC_IP=10.0.0.227/24
CTDB_NATGW_PUBLIC_IFACE=eth0
CTDB_NATGW_DEFAULT_GATEWAY=10.0.0.1
      </pre><p>
	Normally any node in a NATGW group can act as the NATGW
	master.  Some configurations may have special nodes that lack
	connectivity to a public network.  In such cases, those nodes
	can be flagged with the "slave-only" option in the
	<code class="varname">CTDB_NATGW_NODES</code> file to limit the NATGW
	functionality of those nodes.
      </p><p>
	See the <em class="citetitle">NAT GATEWAY</em> section in
	<span class="citerefentry"><span class="refentrytitle">ctdbd.conf</span>(5)</span> for more details of
	NATGW configuration.
      </p></div><div class="refsect2"><a name="idm208"></a><h3>Implementation details</h3><p>
	When the NATGW functionality is used, one of the nodes is
	selected to act as a NAT gateway for all the other nodes in
	the group when they need to communicate with the external
	services.  The NATGW master is selected to be a node that is
	most likely to have usable networks.
      </p><p>
	The NATGW master hosts the NATGW public IP address
	<code class="varname">CTDB_NATGW_PUBLIC_IP</code> on the configured public
	interfaces <code class="varname">CTDB_NATGW_PUBLIC_IFACE</code> and acts as
	a router, masquerading outgoing connections from slave nodes
	via this IP address.  If
	<code class="varname">CTDB_NATGW_DEFAULT_GATEWAY</code> is set then it
	also establishes a fallback default route to the configured
	this gateway with a metric of 10.  A metric 10 route is used
	so it can co-exist with other default routes that may be
	available.
      </p><p>
	A NATGW slave establishes its fallback default route to the
	NATGW master via the private network
	<code class="varname">CTDB_NATGW_PRIVATE_NETWORK</code>with a metric of 10.
	This route is used for outbound connections when no other
	default route is available because the node hosts no public
	addresses.  A metric 10 routes is used so that it can co-exist
	with other default routes that may be available when the node
	is hosting public addresses.
      </p><p>
	<code class="varname">CTDB_NATGW_STATIC_ROUTES</code> can be used to
	have NATGW create more specific routes instead of just default
	routes.
      </p><p>
	This is implemented in the <code class="filename">11.natgw</code>
	eventscript.  Please see the eventscript file and the
	<em class="citetitle">NAT GATEWAY</em> section in
	<span class="citerefentry"><span class="refentrytitle">ctdbd.conf</span>(5)</span> for more details.
      </p></div></div><div class="refsect1"><a name="idm225"></a><h2>POLICY ROUTING</h2><p>
      Policy routing is an optional CTDB feature to support complex
      network topologies.  Public addresses may be spread across
      several different networks (or VLANs) and it may not be possible
      to route packets from these public addresses via the system's
      default route.  Therefore, CTDB has support for policy routing
      via the <code class="filename">13.per_ip_routing</code> eventscript.
      This allows routing to be specified for packets sourced from
      each public address.  The routes are added and removed as CTDB
      moves public addresses between nodes.
    </p><div class="refsect2"><a name="idm229"></a><h3>Configuration variables</h3><p>
	There are 4 configuration variables related to policy routing:
	<code class="varname">CTDB_PER_IP_ROUTING_CONF</code>,
	<code class="varname">CTDB_PER_IP_ROUTING_RULE_PREF</code>,
	<code class="varname">CTDB_PER_IP_ROUTING_TABLE_ID_LOW</code>,
	<code class="varname">CTDB_PER_IP_ROUTING_TABLE_ID_HIGH</code>.  See the
	<em class="citetitle">POLICY ROUTING</em> section in
	<span class="citerefentry"><span class="refentrytitle">ctdbd.conf</span>(5)</span> for more details.
      </p></div><div class="refsect2"><a name="idm240"></a><h3>Configuration</h3><p>
	The format of each line of
	<code class="varname">CTDB_PER_IP_ROUTING_CONF</code> is:
      </p><pre class="screen">
&lt;public_address&gt; &lt;network&gt; [ &lt;gateway&gt; ]
      </pre><p>
	Leading whitespace is ignored and arbitrary whitespace may be
	used as a separator.  Lines that have a "public address" item
	that doesn't match an actual public address are ignored.  This
	means that comment lines can be added using a leading
	character such as '#', since this will never match an IP
	address.
      </p><p>
	A line without a gateway indicates a link local route.
      </p><p>
	For example, consider the configuration line:
      </p><pre class="screen">
  192.168.1.99	192.168.1.1/24
      </pre><p>
	If the corresponding public_addresses line is:
      </p><pre class="screen">
  192.168.1.99/24     eth2,eth3
      </pre><p>
	<code class="varname">CTDB_PER_IP_ROUTING_RULE_PREF</code> is 100, and
	CTDB adds the address to eth2 then the following routing
	information is added:
      </p><pre class="screen">
  ip rule add from 192.168.1.99 pref 100 table ctdb.192.168.1.99
  ip route add 192.168.1.0/24 dev eth2 table ctdb.192.168.1.99
      </pre><p>  
	This causes traffic from 192.168.1.1 to 192.168.1.0/24 go via
	eth2.
      </p><p>
	The <span class="command"><strong>ip rule</strong></span> command will show (something
	like - depending on other public addresses and other routes on
	the system):
      </p><pre class="screen">
  0:		from all lookup local 
  100:		from 192.168.1.99 lookup ctdb.192.168.1.99
  32766:	from all lookup main 
  32767:	from all lookup default 
      </pre><p>
	<span class="command"><strong>ip route show table ctdb.192.168.1.99</strong></span> will show:
      </p><pre class="screen">
  192.168.1.0/24 dev eth2 scope link
      </pre><p>
	The usual use for a line containing a gateway is to add a
	default route corresponding to a particular source address.
	Consider this line of configuration:
      </p><pre class="screen">
  192.168.1.99	0.0.0.0/0	192.168.1.1
      </pre><p>
	In the situation described above this will cause an extra
	routing command to be executed:
      </p><pre class="screen">
  ip route add 0.0.0.0/0 via 192.168.1.1 dev eth2 table ctdb.192.168.1.99
      </pre><p>
	With both configuration lines, <span class="command"><strong>ip route show table
	ctdb.192.168.1.99</strong></span> will show:
      </p><pre class="screen">
  192.168.1.0/24 dev eth2 scope link 
  default via 192.168.1.1 dev eth2 
      </pre></div><div class="refsect2"><a name="idm268"></a><h3>Sample configuration</h3><p>
	Here is a more complete example configuration.
      </p><pre class="screen">
/usr/local/etc/ctdb/public_addresses:

  192.168.1.98	eth2,eth3
  192.168.1.99	eth2,eth3

/usr/local/etc/ctdb/policy_routing:

  192.168.1.98 192.168.1.0/24
  192.168.1.98 192.168.200.0/24	192.168.1.254
  192.168.1.98 0.0.0.0/0 	192.168.1.1
  192.168.1.99 192.168.1.0/24
  192.168.1.99 192.168.200.0/24	192.168.1.254
  192.168.1.99 0.0.0.0/0 	192.168.1.1
      </pre><p>
	The routes local packets as expected, the default route is as
	previously discussed, but packets to 192.168.200.0/24 are
	routed via the alternate gateway 192.168.1.254.
      </p></div></div><div class="refsect1"><a name="idm273"></a><h2>NOTIFICATION SCRIPT</h2><p>
      When certain state changes occur in CTDB, it can be configured
      to perform arbitrary actions via a notification script.  For
      example, sending SNMP traps or emails when a node becomes
      unhealthy or similar.
    </p><p>
      This is activated by setting the
      <code class="varname">CTDB_NOTIFY_SCRIPT</code> configuration variable.
      The specified script must be executable.  
    </p><p>
      Use of the provided <code class="filename">/usr/local/etc/ctdb/notify.sh</code>
      script is recommended.  It executes files in
      <code class="filename">/usr/local/etc/ctdb/notify.d/</code>.
    </p><p>
      CTDB currently generates notifications after CTDB changes to
      these states:
    </p><table border="0" summary="Simple list" class="simplelist"><tr><td>init</td></tr><tr><td>setup</td></tr><tr><td>startup</td></tr><tr><td>healthy</td></tr><tr><td>unhealthy</td></tr></table></div><div class="refsect1"><a name="idm288"></a><h2>DEBUG LEVELS</h2><p>
      Valid values for DEBUGLEVEL are:
    </p><table border="0" summary="Simple list" class="simplelist"><tr><td>ERR</td></tr><tr><td>WARNING</td></tr><tr><td>NOTICE</td></tr><tr><td>INFO</td></tr><tr><td>DEBUG</td></tr></table></div><div class="refsect1"><a name="idm297"></a><h2>REMOTE CLUSTER NODES</h2><p>
It is possible to have a CTDB cluster that spans across a WAN link. 
For example where you have a CTDB cluster in your datacentre but you also
want to have one additional CTDB node located at a remote branch site.
This is similar to how a WAN accelerator works but with the difference 
that while a WAN-accelerator often acts as a Proxy or a MitM, in 
the ctdb remote cluster node configuration the Samba instance at the remote site
IS the genuine server, not a proxy and not a MitM, and thus provides 100%
correct CIFS semantics to clients.
    </p><p>
	See the cluster as one single multihomed samba server where one of
	the NICs (the remote node) is very far away.
    </p><p>
	NOTE: This does require that the cluster filesystem you use can cope
	with WAN-link latencies. Not all cluster filesystems can handle
	WAN-link latencies! Whether this will provide very good WAN-accelerator
	performance or it will perform very poorly depends entirely
	on how optimized your cluster filesystem is in handling high latency
	for data and metadata operations.
    </p><p>
	To activate a node as being a remote cluster node you need to set
	the following two parameters in /etc/sysconfig/ctdb  for the remote node:
        </p><pre class="screen">
CTDB_CAPABILITY_LMASTER=no
CTDB_CAPABILITY_RECMASTER=no
	</pre><p>
    </p><p>
	Verify with the command "ctdb getcapabilities" that that node no longer
	has the recmaster or the lmaster capabilities.
    </p></div><div class="refsect1"><a name="idm305"></a><h2>SEE ALSO</h2><p>
      <span class="citerefentry"><span class="refentrytitle">ctdb</span>(1)</span>,

      <span class="citerefentry"><span class="refentrytitle">ctdbd</span>(1)</span>,

      <span class="citerefentry"><span class="refentrytitle">ctdbd_wrapper</span>(1)</span>,

      <span class="citerefentry"><span class="refentrytitle">ctdb_diagnostics</span>(1)</span>,

      <span class="citerefentry"><span class="refentrytitle">ltdbtool</span>(1)</span>,

      <span class="citerefentry"><span class="refentrytitle">onnode</span>(1)</span>,

      <span class="citerefentry"><span class="refentrytitle">ping_pong</span>(1)</span>,

      <span class="citerefentry"><span class="refentrytitle">ctdbd.conf</span>(5)</span>,

      <span class="citerefentry"><span class="refentrytitle">ctdb-statistics</span>(7)</span>,

      <span class="citerefentry"><span class="refentrytitle">ctdb-tunables</span>(7)</span>,

      <a class="ulink" href="http://ctdb.samba.org/" target="_top">http://ctdb.samba.org/</a>
    </p></div></div></body></html>
